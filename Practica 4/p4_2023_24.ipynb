{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-L43atg5zDU"
   },
   "source": [
    "# Computer Vision - P4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUaLqxZV5zDY"
   },
   "source": [
    "## Delivery\n",
    "\n",
    "Your overall grading will be penalized if the following requirements are not fulfilled:\n",
    "\n",
    "- Implemented code should be commented exhaustively and in *English*.\n",
    "\n",
    "- The questions introduced in the exercises must be answered.\n",
    "\n",
    "- Add title to the figures to explain what is displayed.\n",
    "\n",
    "- Answers to questions also need to be in *English*.\n",
    "\n",
    "- Make sure to print and plot exactly what it is indicated. If a reference image is provided, your output is expected to be exactly the same unless instructed differently.\n",
    "\n",
    "- The deliverable of both parts must be a file named **P4_Student1_Student2.zip** that includes:\n",
    "    - The notebook P4_Student1_Student2.ipynb completed with the solutions to the exercises and their corresponding comments.\n",
    "    - All the images used in this notebook (upload the ones that were not provided)\n",
    "    \n",
    "- It is required that your code can be run by us without need of any modification and without getting any errors.\n",
    "\n",
    "- Use packages and solutions that were covered in your class and tutorials. If you are unsure about using a particular package, you should seek clarification from your instructor to confirm whether it is allowed.\n",
    "\n",
    "- Please refrain from utilizing resources like ChatGPT to complete this lab assignment.\n",
    "\n",
    "\n",
    "**Deadline (Campus Virtual): November 28th, 23:00 h**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBSn094v5zDZ"
   },
   "source": [
    "==============================================================================================\n",
    "## Descriptors extraction for object detection, based on template matching, ORB, and HOG\n",
    "==============================================================================================\n",
    "\n",
    "The main topics of Laboratory 4 are:\n",
    "\n",
    "    4.1) SSD and Normalized Cross-correlation for template matching\n",
    "\n",
    "    4.2) HOG image descriptor for object (person) detection\n",
    "\n",
    "    4.3) Recognition by correspondance, based on feature extraction (ORB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJ2goxOz5zDa"
   },
   "source": [
    "In order to complete this practicum, the following concepts need to be understood: template matching, feature localization (Harris, Censure), feature descriptor (HOG,ORB, Sift) methods.\n",
    "\n",
    "It is highly recommendable to structure the code in functions in order to reuse code for different tests and images and make it shorter and more readable. Specially the visualization commands should be encapsulated in separate functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RRidrtq5zDa"
   },
   "source": [
    "## 1 Template matching\n",
    "\n",
    "**1.1 (10 points)** Given the image 'einstein.png' and the template image 'eye.png', detect the location of the template in the image comparing the use of:\n",
    "- SSD distance (hint: norm() in numpy.linalg).\n",
    "- normalized cross-correlation (hint: match_template() of skimage.feature).\n",
    "\n",
    "Don't forget to normalize the images (having pixel values between [0,1]) before comparing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GZtgH7r5zDb",
    "outputId": "140d0d9c-b444-47e0-ccd3-c0ac62ddd2fd"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3QP_6Fh5zDe"
   },
   "source": [
    "Create a **function** for template matching applying SSD distance and normalized cross-correlation and display the results in the following format:\n",
    "\n",
    "<img src=\"images/tm.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "**Use titles in all figures to understand what is being displayed.**\n",
    "\n",
    "Note: the decision of threshold for SSD distance and normalized cross-correlation will affect your results, try values that get you as close to the shown result as possible but we understand you might not get exactly the same outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88YH-3gj5zDf",
    "outputId": "11cff94a-9c14-4419-c358-6f0a3ce200ec",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tc6M7DSr5zDg"
   },
   "source": [
    "**1.2 (5 points)** How does the result of the template matching change if the image changes its contrast (for example if you make it clearer or darker)?\n",
    "\n",
    "Similarly to the previous case, please, visualize the euclidean distance and normalized cross-correlation images as well as the binarized (thresholded) images in the two cases.\n",
    "\n",
    "**Note:** Use titles of the figures to explain what is displayed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vieqwc4-5zDg"
   },
   "source": [
    "Read `einstein_br.png` and display the results of the template matching techniques used on 1.1 on this image, using `eye.png` as template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gjGIkr85zDh",
    "outputId": "3e45a1b8-1d3e-4832-dedd-66cc61b01410"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "md3XFIGY5zDh"
   },
   "source": [
    "Print the minimum SSD and the maximum Normalized Cross-Correlation (NCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPF-KYtL5zDh",
    "outputId": "516b9dd5-c8da-418e-d0f1-039e9519c6e0"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmSqfmIp5zDi"
   },
   "source": [
    "**1.3 (5 points)** Read `einstein_mask.png` and display the results on this image using your previously coded function, using the `eye.png` template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92U_iN5U5zDi",
    "outputId": "cfd4f688-949d-4a80-e8f2-ff1b3b0a2f71"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnimrwLz5zDj"
   },
   "source": [
    "Print the minimum SSD and the maximum NCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmJSuQy-5zDj",
    "outputId": "31f66b36-0a27-4d03-cd6e-a5ea58411572"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHaduRYi5zDj"
   },
   "source": [
    "What are the distances between the template and the image around the eyes of the image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwbODscY5zDk",
    "outputId": "058a7d01-5e2e-4825-ee41-e3409f389f81"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQpwzg9v5zDk"
   },
   "source": [
    "Comment the template matching process:\n",
    "* Is the algorithms affected by contrast changes in the image?\n",
    "* How do metrics (i.e. minimum euclidean distance and maximum NCC) change in all previous cases? Is there a big difference among these values?\n",
    "* What parameters it has and which measure for image comparisons works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIyNGpsp5zDl"
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5juYaFZx5zDl"
   },
   "source": [
    "**1.4 (5 points)** How does the result of the template matching change if instead the template is the one that changes its contrast (for example if you make it clearer or darker)? To this purpose, use the `eye_br.png` template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b53XkiS25zDm",
    "outputId": "1d9a3207-2998-4441-def3-eebd85fb2f3c"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ev4HxShF5zDm"
   },
   "source": [
    "Check how the result changes if the template is rotated.\n",
    "\n",
    "Visualize the template and its rotation by 2º, 5º, 10º, 15º and 20º. Obtain again the template matching using the Euclidean distance and normalized cross-correlation. Pick fixed values of thresholds that are well suited for the non rotated image.  \n",
    "\n",
    "**Help:** use the function rotate() in skimage.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SD8bqZ8F5zDm",
    "outputId": "53c55d28-f1fc-4a0e-d4fb-9bec25720960"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGf_O33H5zDn"
   },
   "source": [
    "Comment the template matching process:\n",
    "* Please, explain briefly the algorithm, including advantages and disadvantage\n",
    "* Is the algorithms affected by contrast changes in the tempate image?\n",
    "* What parameters it has and which measure for image comparisons works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lq4Nwphw5zDn"
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoXYqqtt5zDn"
   },
   "source": [
    "# 2 Histogram of Oriented Gradients\n",
    "\n",
    "In this section we will treat the following topic: HOG image descriptor. Application to person detection.\n",
    "\n",
    "The Histogram of Oriented Gradients (HOG) feature descriptor is classical image descriptor for object detection.\n",
    "\n",
    "Given the image `person_template.bmp` and the folder `/images/TestPersonImages/`, apply the HOG descriptor in order to detect where there is a person in the images. To this purpose, apply the \"sliding window\" technique. We use images from GRAZ 01 data from [INRIA datasets](http://pascal.inrialpes.fr/data/human/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBQvHiS65zDn"
   },
   "source": [
    "**2.1. (5 points)** Read the template (`person_template.bmp`), obtain its HOG descriptor (with the optimal parameters) and visualize it.\n",
    "\n",
    "Help: the HOG detector function is in the skimage.feature library ([Help](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html#sphx-glr-auto-examples-features-detection-plot-hog-py)).\n",
    "\n",
    "Obtain 2 versions of the HOG descriptor:\n",
    "- With the default parameters.\n",
    "- Then with 4x4 pixels per cell and 2x2 cells per block and orientations set to 8\n",
    "\n",
    "Visualize the original image, the default values HOG descriptor and the defined parameters HOG descriptor in a 1x3 grid of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pj3ipHGx5zDn",
    "outputId": "1d21514b-5dbe-4162-c021-9f4afd23bc97"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tej4SeWL5zDo"
   },
   "source": [
    "Print and explain the number of feature resulting on the default parameters HOG descriptor (4536). Break this number down, starting from the original image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnzsBW7x5zDo",
    "outputId": "144d192c-d459-4563-82c8-4aa4438ee9d0"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "ZIzT-xzY5zDo"
   },
   "source": [
    "**2.2. (15 points)** Apply the HOG descriptor on the complete set of images for person detection.\n",
    "\n",
    "a) Read images from the folder \"TestPersonImages\", slide a window on each image, obtain the HOG descriptor and compare to the HOG descriptor of the person template.\n",
    "\n",
    "b) Visualize the location in the image that is the most similar to the person template using the distance between the template and test image descriptors.\n",
    "\n",
    "Display the results of every person detection following this format:\n",
    "\n",
    "<img src=\"images/hog.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13vXOBIP5zDo",
    "outputId": "aa71e115-37e9-4363-b6ba-a3d6133c182f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKs4F7CU5zDp"
   },
   "source": [
    "Count on how many images were the persons detected correctly and discuss the failures.\n",
    "What do you think can be the reasons for the failures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqL8OHcx5zDp"
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jh1PzzA05zDp"
   },
   "source": [
    "Test several values of the parameters (``orientations``, ``pixels_per_cell``, ``cells_per_block``) to show which are the optimal values for the person detection problem. Note: since you are testing several values and not the whole universe, this question might have different solutions, make sure to try several to at least be directionally correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiAsf4PN5zDp"
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFY-STIS5zDy"
   },
   "source": [
    "Comment the object detection process:\n",
    "* Please, explain briefly the algorithm, including advantages and disadvantage\n",
    "* Do you see any advantages of the HOG-based object detector compared to the template-based object detection? (The answer should be up to 10-15 lines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uazlbxLQ5zDz"
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZydeHzO5zDz"
   },
   "source": [
    "# 3 ORB feature detector and binary descriptor\n",
    "\n",
    "Let us consider the problem of feature extraction that contains two subproblems:\n",
    "- feature location,\n",
    "- image feature description.\n",
    "\n",
    "Let us focus on ORB, an approximation of SIFT method, and analyse if ORB is  scale and rotation invariant, a property that is very important for real-time applications.\n",
    "\n",
    "**Hint:** `ORB` is a function within the module `skimage.feature`\n",
    "                             \n",
    "**Help**: We suggest to have a look at the [ORB example](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_orb.html) how to compute the ORB descriptors and find the descriptors match. You can use the function match_descriptors from `skimage.feature` module in order to compute and show the similar detected descriptors of the given images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsmNBJnb5zDz"
   },
   "source": [
    "**3.1 (10 points)** First, detect the censure in the image `starbucks4.jpg`. Analyze and discuss the effect of different values of the parameters in censure function. (this is a separate funcion from ORB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6o9DxiDp5zDz",
    "outputId": "190c2387-f49c-43cd-8a89-c7cd41ab6567"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaclrZOv5zD0"
   },
   "source": [
    "**3.2 (10 points)** Detect the correspondences between the model image `starbucks.jpg` with the scene image `starbucks4.jpg`. You can adapt the code from the [ORB example](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_orb.html).\n",
    "\n",
    "Define a function get_ORB implementing the algorithm in order to be able to apply it on different images. Comment the code in detail.\n",
    "\n",
    "**Hint: If the function plot_matches() gives you an error you can use the plot_matches_aux() at the end of this file.**\n",
    "\n",
    "Analyze and discuss the effect of different values of the parameter `max_ratio` in the match_descriptors function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgefyPlv5zD0",
    "outputId": "44170f0c-a76c-48b1-e00f-9a4038d0561b"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQa1sbDQ5zD0"
   },
   "source": [
    "Repeate the experiment comparing the `starbucks.jpg` image as a model, and showing its matches to all Starbucks images, sorting them based on their similarity to the model. Comment when does the algorithm work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68m0d-QX5zD0",
    "outputId": "9df9395b-96dd-4aab-8b2e-f73f743a1d51"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPBEjFSd5zD1"
   },
   "source": [
    "**3.3 (5 points)** Repeat the experiment:\n",
    "- Changing the orientation of the model image by rotating it and comparing it with its original version. Help: you can use the rotate() function from skimage.transform\n",
    "- Change the scale and orientation of the scene image and compare it with the model image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kIboi9b5zD1"
   },
   "source": [
    "**Help:** To do so, you can use the function given below as example:\n",
    "\n",
    "```\n",
    "import transform as tf\n",
    "rotationdegrees = 180\n",
    "img_rotated = tf.rotate(image2transform, rotationdegrees)\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "tform = tf.AffineTransform(scale=(1.2, 1.2), translation=(0, -100), rotation=0.5)\n",
    "img_transformed = tf.warp(image2transform, tform)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXYBVroJ5zD1",
    "outputId": "af7937c3-503b-4565-e5ea-df4fd63b1ac9"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg0lfAED5zD1"
   },
   "source": [
    "**(Optional)** Repeat the experiment (3.1 to 3.3) with a new group of images. You could use Coca-Cola advertisements or from another famous brand, easily to find on internet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYF36r_45zD2"
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zCjx-th5zD2"
   },
   "source": [
    "**3.4 (5 points)** Analysis of the applied techniques and results\n",
    "\n",
    "- What are the advantages of the ORB object detection with respect to the HOG and template object detector?\n",
    "\n",
    "- What would happen if you analyse an image that does not contain the Starbucks logo?\n",
    "\n",
    "- Could you think of ways of defining a quality measure for the correspondance between two images? (no need of implementing it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3WKaztS5zD2"
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvalWtfg5zD3"
   },
   "source": [
    "# 4. SIFT\n",
    "\n",
    "**4.1 (5 points)** Now use SIFT to the same problem statement as 3.2 exercise, make sure to show the results with all starbucks images provided. For now, you can use the default parameters of SIFT.\n",
    "* Compare the results and the running time of both algortihms.\n",
    "* Which one works best on this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0PnLm4h5zD3",
    "outputId": "a566d763-0439-4fa5-d36d-f5806121b2d3"
   },
   "outputs": [],
   "source": [
    "# Your solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xe0ePKF5zD3"
   },
   "source": [
    "**4.2 (10 points)** Analyze the different SIFT parameters. Try and show the results for at least 3 different combination of parameters for starbucks4.jpg image against starbucks model image.\n",
    "\n",
    "* What are their effect?\n",
    "* How do they compare to ORB parameters? \n",
    "* Which algorithm is more suitable for images with rotation? And for images that have been rescaled, which one is more effective?\n",
    "* When it comes to changes in image intensity, which algorithm performs better, ORB or SIFT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNcZmvQ15zD3",
    "outputId": "5c785be9-95f0-4435-ce81-9139018003c0"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3 (10 points)** Can we build an image classifier using the SIFT algorithm? Start by uploading the images from the folder `matches_SIFT`, then use the SIFT algorithm to find the most similar pairs (use the following parameters for this: max_ratio=0.8, upsampling=2, n_octaves=8, n_scales=3). To compare all the images, we'll need to establish a metric. We suggest to use the number of matches between images.\n",
    "\n",
    "Show your results (SIFT result image matches and print the similarity only for the most similar image for each of the 6 images provided). Rescale the images to see the matches between images.\n",
    "\n",
    "Answer the following: \n",
    "* Is the algorithm failing to find accurate matches (similar images) for some cases? If so, why do you think this is happening? You can assume that a good results is that eiffel tower images match between them and so on. \n",
    "\n",
    "Here is an example of the expected results for ciudad_ciencias_1.jpg (you also have to print the similarity metric result): \n",
    "\n",
    "<img src=\"images/example_city_of_arts.png\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSL87S_d5zD2"
   },
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5BFJwXz5zD2"
   },
   "outputs": [],
   "source": [
    "# In case the plot_matches() function gives you some problems, you can use the following one:\n",
    "\n",
    "from skimage.util import img_as_float\n",
    "import numpy as np\n",
    "\n",
    "def plot_matches_aux(ax, image1, image2, keypoints1, keypoints2, matches,\n",
    "                 keypoints_color='k', matches_color=None, only_matches=False):\n",
    "    \"\"\"Plot matched features.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        Matches and image are drawn in this ax.\n",
    "    image1 : (N, M [, 3]) array\n",
    "        First grayscale or color image.\n",
    "    image2 : (N, M [, 3]) array\n",
    "        Second grayscale or color image.\n",
    "    keypoints1 : (K1, 2) array\n",
    "        First keypoint coordinates as ``(row, col)``.\n",
    "    keypoints2 : (K2, 2) array\n",
    "        Second keypoint coordinates as ``(row, col)``.\n",
    "    matches : (Q, 2) array\n",
    "        Indices of corresponding matches in first and second set of\n",
    "        descriptors, where ``matches[:, 0]`` denote the indices in the first\n",
    "        and ``matches[:, 1]`` the indices in the second set of descriptors.\n",
    "    keypoints_color : matplotlib color, optional\n",
    "        Color for keypoint locations.\n",
    "    matches_color : matplotlib color, optional\n",
    "        Color for lines which connect keypoint matches. By default the\n",
    "        color is chosen randomly.\n",
    "    only_matches : bool, optional\n",
    "        Whether to only plot matches and not plot the keypoint locations.\n",
    "    \"\"\"\n",
    "\n",
    "    image1 = img_as_float(image1)\n",
    "    image2 = img_as_float(image2)\n",
    "\n",
    "    new_shape1 = list(image1.shape)\n",
    "    new_shape2 = list(image2.shape)\n",
    "\n",
    "    if image1.shape[0] < image2.shape[0]:\n",
    "        new_shape1[0] = image2.shape[0]\n",
    "    elif image1.shape[0] > image2.shape[0]:\n",
    "        new_shape2[0] = image1.shape[0]\n",
    "\n",
    "    if image1.shape[1] < image2.shape[1]:\n",
    "        new_shape1[1] = image2.shape[1]\n",
    "    elif image1.shape[1] > image2.shape[1]:\n",
    "        new_shape2[1] = image1.shape[1]\n",
    "\n",
    "    if new_shape1 != image1.shape:\n",
    "        new_image1 = np.zeros(new_shape1, dtype=image1.dtype)\n",
    "        new_image1[:image1.shape[0], :image1.shape[1]] = image1\n",
    "        image1 = new_image1\n",
    "\n",
    "    if new_shape2 != image2.shape:\n",
    "        new_image2 = np.zeros(new_shape2, dtype=image2.dtype)\n",
    "        new_image2[:image2.shape[0], :image2.shape[1]] = image2\n",
    "        image2 = new_image2\n",
    "\n",
    "    image = np.concatenate([image1, image2], axis=1)\n",
    "\n",
    "    offset = image1.shape\n",
    "\n",
    "    if not only_matches:\n",
    "        ax.scatter(keypoints1[:, 1], keypoints1[:, 0],\n",
    "                   facecolors='none', edgecolors=keypoints_color)\n",
    "        ax.scatter(keypoints2[:, 1] + offset[1], keypoints2[:, 0],\n",
    "                   facecolors='none', edgecolors=keypoints_color)\n",
    "\n",
    "    ax.imshow(image, interpolation='nearest', cmap='gray')\n",
    "    ax.axis((0, 2 * offset[1], offset[0], 0))\n",
    "\n",
    "    for i in range(matches.shape[0]):\n",
    "        idx1 = matches[i, 0]\n",
    "        idx2 = matches[i, 1]\n",
    "\n",
    "        if matches_color is None:\n",
    "            color = np.random.rand(3)\n",
    "        else:\n",
    "            color = matches_color\n",
    "\n",
    "        ax.plot((keypoints1[idx1, 1], keypoints2[idx2, 1] + offset[1]),\n",
    "                (keypoints1[idx1, 0], keypoints2[idx2, 0]),\n",
    "                '-', color=color)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
